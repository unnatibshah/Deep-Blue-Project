{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowd Counting by Estimating a Density Map With Convolutional Neural Networks\n",
    "\n",
    "The main idea is to count objects indirectly by estimating a density map. The first step is to prepare training samples, so that for every image there is a corresponding density map. \n",
    "\n",
    "![image.png](https://miro.medium.com/max/875/1*kZ6LcW9VQx8v27kTHBFPZA.png)\n",
    "\n",
    "The image presented in Fig. 2 (left) is annotated with points in the positions of pedestrians heads (Fig. 2 right). A density map is obtained by applying a convolution with a Gaussian kernel (and normalized so that integrating it gives the number of objects). The density map for the example above is presented in Fig. 3.\n",
    "\n",
    "![image1.png](https://miro.medium.com/max/875/1*BwFvVyyQW6hxp6K-5dG3Lg.png)\n",
    "\n",
    "Now, the goal is to train a fully convolutional network to map an image to a density map, which can be later integrated to get the number of objects. So far, we have considered two FCN architectures: U-Net [8] and Fully Convolutional Regression Network (FCRN) [7].\n",
    "\n",
    "### U-Net\n",
    "\n",
    "U-Net is a widely used FCN for image segmentation, very often applied to biomedical data. It has autoencoder-like structure (see Fig. 4). An input image is processed by a block of convolutional layers, followed by a pooling layer (downsampling). This procedure is repeated several times on subsequent blocks outputs, which is demonstrated on the left side of Fig. 4. This way the network encodes (and compresses) the key features of an input image. The second part of U-Net is symmetric, but pooling layers are replaced with upsampling, so that an output dimensions match the size of an input image. The information from higher resolution layers in the downsampling part is passed to corresponding layers in the upsampling part, which allows to reuse learned higher level features to decode contracted layers more precisely.\n",
    "\n",
    "![image2.png](https://miro.medium.com/max/875/1*_6mXlLMKU0Vf1IJjBn7xkQ.png)\n",
    "\n",
    "### FCRN\n",
    "\n",
    "Fully Convolutional Regression Network (FCRN) was proposed in [7]. The architecture is very similar to U-Net. The main difference is that the information from higher resolution layers from downsampling part is not passed directly to the corresponding layers in upsampling part. In the paper two networks are proposed: FCRN-A and FCRN-B, which differ in downsampling intensity. While FCRN-A perform pooling every convolutional layer, FCRN-B does that every second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, U-Net and FCRN-A are implemented. They both use three downsampling and three upsampling convolutional blocks with fixed filter size 3Ã—3. By default there are two convolutional layers in each block for U-Net, and one for FCRN-A. For U-Net we keep constant number of filters for all convolutional layers, and for FCRN-A we increase this number every subsequent layer to compensate for the loss of higher resolution information caused by pooling (which is not passed directly as in the case of U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic piece to build both U-Net and FCRN is a convolutional block, consisting of a convolutional layer, batch normalization, and activation function:\n",
    "The conv_block function creates N convolutional layers with OUT number of filters with ReLU activation function and batch normalization applied in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(channels: Tuple[int, int],\n",
    "               size: Tuple[int, int],\n",
    "               stride: Tuple[int, int]=(1, 1),\n",
    "               N: int=1):\n",
    "    \"\"\"\n",
    "    Create a block with N convolutional layers with ReLU activation function.\n",
    "    The first layer is IN x OUT, and all others - OUT x OUT.\n",
    "    Args:\n",
    "        channels: (IN, OUT) - no. of input and output channels\n",
    "        size: kernel size (fixed for all convolution in a block)\n",
    "        stride: stride (fixed for all convolution in a block)\n",
    "        N: no. of convolutional layers\n",
    "    Returns:\n",
    "        A sequential container of N convolutional layers.\n",
    "    \"\"\"\n",
    "    # a single convolution + batch normalization + ReLU block\n",
    "    block = lambda in_channels: nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels,\n",
    "                  out_channels=channels[1],\n",
    "                  kernel_size=size,\n",
    "                  stride=stride,\n",
    "                  bias=False,\n",
    "                  padding=(size[0] // 2, size[1] // 2)),\n",
    "        nn.BatchNorm2d(num_features=channels[1]),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    # create and return a sequential container of convolutional layers\n",
    "    # input size = channels[0] for first block and channels[1] for all others\n",
    "    return nn.Sequential(*[block(channels[bool(i)]) for i in range(N)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCRN-A architecture\n",
    "\n",
    "The FCRN-A architecture is obtained by stacking multiple such blocks followed by either downsampling (max pooling) or upsampling layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCRN_A(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Convolutional Regression Network A\n",
    "    Ref. W. Xie et al. 'Microscopy Cell Counting with Fully Convolutional\n",
    "    Regression Networks'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, N: int=1, input_filters: int=3, **kwargs):\n",
    "        \"\"\"\n",
    "        Create FCRN-A model with:\n",
    "            * fixed kernel size = (3, 3)\n",
    "            * fixed max pooling kernel size = (2, 2) and upsampling factor = 2\n",
    "            * no. of filters as defined in an original model:\n",
    "              input size -> 32 -> 64 -> 128 -> 512 -> 128 -> 64 -> 1\n",
    "        Args:\n",
    "            N: no. of convolutional layers per block (see conv_block)\n",
    "            input_filters: no. of input channels\n",
    "        \"\"\"\n",
    "        super(FCRN_A, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # downsampling\n",
    "            conv_block(channels=(input_filters, 32), size=(3, 3), N=N),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            conv_block(channels=(32, 64), size=(3, 3), N=N),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            conv_block(channels=(64, 128), size=(3, 3), N=N),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # \"convolutional fully connected\"\n",
    "            conv_block(channels=(128, 512), size=(3, 3), N=N),\n",
    "\n",
    "            # upsampling\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            conv_block(channels=(512, 128), size=(3, 3), N=N),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            conv_block(channels=(128, 64), size=(3, 3), N=N),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            conv_block(channels=(64, 1), size=(3, 3), N=N),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net architecture\n",
    "\n",
    "U-Net also requires to concatenate the output from the downsampling path with the input to the corresponding layer in the upsampling part, which is performed by ConvCat class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCat(nn.Module):\n",
    "    \"\"\"Convolution with upsampling + concatenate block.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels: Tuple[int, int],\n",
    "                 size: Tuple[int, int],\n",
    "                 stride: Tuple[int, int]=(1, 1),\n",
    "                 N: int=1):\n",
    "        \"\"\"\n",
    "        Create a sequential container with convolutional block (see conv_block)\n",
    "        with N convolutional layers and upsampling by factor 2.\n",
    "        \"\"\"\n",
    "        super(ConvCat, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv_block(channels, size, stride, N),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, to_conv: torch.Tensor, to_cat: torch.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            to_conv: input passed to convolutional block and upsampling\n",
    "            to_cat: input concatenated with the output of a conv block\n",
    "        \"\"\"\n",
    "        return torch.cat([self.conv(to_conv), to_cat], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net implementation.\n",
    "    Ref. O. Ronneberger et al. \"U-net: Convolutional networks for biomedical\n",
    "    image segmentation.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters: int=64, input_filters: int=3, **kwargs):\n",
    "        \"\"\"\n",
    "        Create U-Net model with:\n",
    "            * fixed kernel size = (3, 3)\n",
    "            * fixed max pooling kernel size = (2, 2) and upsampling factor = 2\n",
    "            * fixed no. of convolutional layers per block = 2 (see conv_block)\n",
    "            * constant no. of filters for convolutional layers\n",
    "        Args:\n",
    "            filters: no. of filters for convolutional layers\n",
    "            input_filters: no. of input channels\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        # first block channels size\n",
    "        initial_filters = (input_filters, filters)\n",
    "        # channels size for downsampling\n",
    "        down_filters = (filters, filters)\n",
    "        # channels size for upsampling (input doubled because of concatenate)\n",
    "        up_filters = (2 * filters, filters)\n",
    "\n",
    "        # downsampling\n",
    "        self.block1 = conv_block(channels=initial_filters, size=(3, 3), N=2)\n",
    "        self.block2 = conv_block(channels=down_filters, size=(3, 3), N=2)\n",
    "        self.block3 = conv_block(channels=down_filters, size=(3, 3), N=2)\n",
    "\n",
    "        # upsampling\n",
    "        self.block4 = ConvCat(channels=down_filters, size=(3, 3), N=2)\n",
    "        self.block5 = ConvCat(channels=up_filters, size=(3, 3), N=2)\n",
    "        self.block6 = ConvCat(channels=up_filters, size=(3, 3), N=2)\n",
    "\n",
    "        # density prediction\n",
    "        self.block7 = conv_block(channels=up_filters, size=(3, 3), N=2)\n",
    "        self.density_pred = nn.Conv2d(in_channels=filters, out_channels=1,\n",
    "                                      kernel_size=(1, 1), bias=False)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # use the same max pooling kernel size (2, 2) across the network\n",
    "        pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # downsampling\n",
    "        block1 = self.block1(input)\n",
    "        pool1 = pool(block1)\n",
    "        block2 = self.block2(pool1)\n",
    "        pool2 = pool(block2)\n",
    "        block3 = self.block3(pool2)\n",
    "        pool3 = pool(block3)\n",
    "\n",
    "        # upsampling\n",
    "        block4 = self.block4(pool3, block3)\n",
    "        block5 = self.block5(block4, block2)\n",
    "        block6 = self.block6(block5, block1)\n",
    "\n",
    "        # density prediction\n",
    "        block7 = self.block7(block6)\n",
    "        return self.density_pred(block7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PYTESTS --- #\n",
    "\n",
    "def run_network(network: nn.Module, input_channels: int):\n",
    "    \"\"\"Generate a random image, run through network, and check output size.\"\"\"\n",
    "    sample = torch.ones((1, input_channels, 224, 224))\n",
    "    result = network(input_filters=input_channels)(sample)\n",
    "    assert result.shape == (1, 1, 224, 224)\n",
    "\n",
    "\n",
    "def test_UNet_color():\n",
    "    \"\"\"Test U-Net on RGB images.\"\"\"\n",
    "    run_network(UNet, 3)\n",
    "\n",
    "\n",
    "def test_UNet_grayscale():\n",
    "    \"\"\"Test U-Net on grayscale images.\"\"\"\n",
    "    run_network(UNet, 1)\n",
    "\n",
    "\n",
    "def test_FRCN_color():\n",
    "    \"\"\"Test FCRN-A on RGB images.\"\"\"\n",
    "    run_network(FCRN_A, 3)\n",
    "\n",
    "\n",
    "def test_FRCN_grayscale():\n",
    "    \"\"\"Test FCRN-A on grayscale images.\"\"\"\n",
    "    run_network(FCRN_A, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
